{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Modelling for the Customer Churn Problem Statement\n",
    "\n",
    "## Approach\n",
    "### Salient points to note from EDA : \n",
    "1. The data set is highly imbalanced. So, have to try out sampling techniques etc.\n",
    "* ~9% positive class\n",
    "2. No real correlation of the status variable with other given variables\n",
    "3. NA values are present in last few predictors provided\n",
    "4. Only 37 dates are present.\n",
    "\n",
    "\n",
    "## Steps \n",
    "1. **Cleaning the dataset**. \n",
    "  * The given data has duplicates. Remove them\n",
    "  * Convert the data into right formats\n",
    "2. **Data imputation**. Decide what to do with the rows with NA values.\n",
    "  + As a first step, \"deletion\" can be used as an imputation technique and the performance characteristic measured.\n",
    "  + Secondly, median / mode values of the respective predictors be used as the imputed values. Mean is not used as there are lot of outliers.\n",
    "3. Fix a **performance characteristic**. \n",
    "  * Overall Accuracy is not a suitable measure due to imbalance in the dataset. Although, accuracy can be looked at after sampling is done.\n",
    "  * AUC can be the primary characteristic.\n",
    "  * Secondarily. F1 Score/Confusion Matrix can be used a performance measure.\n",
    "2. **Benchmarking**. Build a few simple models just by using the given predictors\n",
    "2. **Outlier Treatment**. \n",
    "  * Either remove the outliers or truncate them to a suitable value\n",
    "3. **Feature creation**. Create as many features as possible and then converge. Some of the possible features are\n",
    "  + Caputre \"month\", \"isWeekend\",\"dayname\" from \"asOfDate\" to infer temporal information.\n",
    "  + EDA showed some slight cyclical variation with date. Maybe created a weight for high-churn rate months\n",
    "  + A toggle variable that indicates Postive / Negative change from prevHits, prevLogin, prevTickets to current values.\n",
    "5. **Create Validation Set, Final Test Set**.\n",
    "  * Final set should be created before the sampling process and kept aside.\n",
    "  * Ratio followed will be 70:15:15\n",
    "  * Make sure test set have enough number of positive class. \n",
    "  * So, manually select 15% random positive class (~ 800 ) and 15% negative class for\n",
    "\n",
    "3. **Data Preparation**. Since the dataset is highly imbalanced, following techniques have to be used.\n",
    "  * Customised Undersampling of the majority class (Status = 0). Customised because, several clusters (10 in this case) will be created. This is so that information loss does not happen. 10 different models can be trained and them ensembled in the end.\n",
    "  * Oversampling the minority class\n",
    "  * Cost sensitive learning - Penalise False negatives more.\n",
    "6. **Estimation of Performance**\n",
    "  * Create a model quickly and do k=7 means cross validation\n",
    "6. **Variable Importance**\n",
    "  * Use the importance flag in random forest and importance \n",
    "7. **Hyper-parameter tuning** \n",
    "  * Only manual parameter tuning done. \n",
    "  * Hyper-parameter tuning using gradient \n",
    "8. **Model Ensembling**\n",
    "  * Combined the predictions of several models to take the majority outcome\n",
    "9. **Reporting performance measures*\n",
    "\n",
    "|               Trial Description              |   AUC  | F1 Score | Accuracy |\n",
    "|:--------------------------------------------:|:------:|:--------:|:--------:|\n",
    "| NA omitted - Benchmark 1                     | 0.5006 | 0.95     | 90.77%   |\n",
    "| NA omitted - Benchmark 2                     | 0.6752 | 0.64     | 67.66%   |\n",
    "| NA omitted - Outlier truncated - Benchmark 1 | 0.5005 | 0.95     | 91.20%   |\n",
    "| NA omitted - Outlier truncated - Benchmark 2 | 0.6605 | 0.62     | 66.13%   |\n",
    "| NA imputed - Benchmark 1                     | 0.5047 | 0.95     | 90.70%   |\n",
    "| NA imputed - Outlier truncated - Benchmark 2 | 0.6675 | 0.64     | 66.70%   |\n",
    "| NA imputed - Outlier truncated - Benchmark 1 | 0.5041 | 0.95     | 90.70%   |\n",
    "| NA imputed - Benchmark 2                     | 0.6665 | 0.63     | 66.80%   |\n",
    "| Feature Engineered RF                        | 0.6805 | 0.65     | 68.07%   |\n",
    "| Feature Engineered XgB nrounds = 300         | 0.6554 | 0.64     | 65.55%   |\n",
    "| Feature Engineered XgB nrounds = 10000       | 0.6495 | 0.64     | 64.93%   |\n",
    "| Ensembling models                            | 0.6837 | 0.72     | 60.40%   |\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "packages <- c(\"ggplot2\", \"dplyr\", \"ranger\", \"xgboost\", \"caret\", \"e1071\",\"lubridate\")\n",
    "if (length(setdiff(packages, rownames(installed.packages()))) > 0) {\n",
    "  install.packages(setdiff(packages, rownames(installed.packages())))  \n",
    "}\n",
    "\n",
    "# Load the necessary libraries and the dataset\n",
    "library(ranger)\n",
    "library(ggplot2)\n",
    "library(caret)\n",
    "library(xgboost)\n",
    "library(lubridate)\n",
    "\n",
    "\n",
    "########## Required Functions ##############\n",
    "\n",
    "checkPerformance <- function(pred,y) {\n",
    "  \n",
    "  cm <- confusionMatrix(pred,y)\n",
    "  precision <- posPredValue(pred, y)\n",
    "  recall <- sensitivity(pred, y)\n",
    "  F1 <- (2 * precision * recall) / (precision + recall)\n",
    "  p_obj <- prediction(as.numeric(pred),as.numeric(y))\n",
    "  auc <- performance(p_obj,measure = \"auc\")@y.values\n",
    "  perf.list <- list(\"confusionMatrix\" = cm, \"F1 score\" = F1, \"Accuracy\" = cm$overall[1], \"AUC\" = auc)\n",
    "  return(perf.list)\n",
    "}\n",
    "\n",
    "\n",
    "########### Load the dataset ###############\n",
    "\n",
    "\n",
    "setwd(\"~/churner\")\n",
    "master <- read.csv(\"masterdata_dummy.csv\")\n",
    "str(master)\n",
    "\n",
    "\n",
    "########### Data preparation ###############\n",
    "#Remove duplicates\n",
    "master <- unique(master)\n",
    "#Convert the dates\n",
    "master$asOfDate <- as.Date(master$asOfDate)\n",
    "#status is a factor.\n",
    "master$status <- as.factor(master$status)\n",
    "\n",
    "\n",
    "########### Data imputation ################\n",
    "\n",
    "#As a first step, just remove the data with NA values\n",
    "#master <- na.omit(master)\n",
    "#To include the information from the NA valued observation, apply the median value to NA\n",
    "#To provide more information, imputed values for positive and negative class to be diff\n",
    "med_p <- lapply(na.omit(master[master$status == 1,18:23]), median)\n",
    "med_n <- lapply(na.omit(master[master$status == 0,18:23]), median)\n",
    "\n",
    "master[is.na(master$ActiveDistinctContacts) & master$status == 1,18] <- med_p[1]\n",
    "master[is.na(master$emailSent) & master$status == 1,19] <- med_p[2]\n",
    "master[is.na(master$Tasks) & master$status == 1,20] <- med_p[3]\n",
    "master[is.na(master$Tags) & master$status == 1,21] <- med_p[4]\n",
    "master[is.na(master$Opportunities) & master$status == 1,22] <- med_p[5]\n",
    "master[is.na(master$WebForms) & master$status == 1,23] <- med_p[6]\n",
    "\n",
    "master[is.na(master$ActiveDistinctContacts) & master$status == 0,18] <- med_p[1]\n",
    "master[is.na(master$emailSent) & master$status == 0,19] <- med_n[2]\n",
    "master[is.na(master$Tasks) & master$status == 0,20] <- med_n[3]\n",
    "master[is.na(master$Tags) & master$status == 0,21] <- med_n[4]\n",
    "master[is.na(master$Opportunities) & master$status == 0,22] <- med_n[5]\n",
    "master[is.na(master$WebForms) & master$status == 0,23] <- med_n[6]\n",
    "\n",
    "summary(master[18:23])\n",
    "\n",
    "\n",
    "########### Benchmarking [very primitive models on the entire dataset] #####\n",
    "\n",
    "#Benchmark - 1\n",
    "#Split the data into two sets 80:20::train:test\n",
    "\n",
    "index <- sample(1:nrow(master),round(0.8*nrow(master)))\n",
    "b_train <- master[index,]\n",
    "b_test <- master[-index,]\n",
    "\n",
    "#Run a sample random forest alogrithm on the entire dataset. \n",
    "rf.bench1 <- ranger(data = b_train[,3:24],dependent.variable.name = \"status\",num.trees = 300,write.forest = T)\n",
    "rf.pred1 <- predict(rf.bench1,data = b_test[,3:24])\n",
    "\n",
    "#Performance on the validation set\n",
    "checkPerformance(rf.pred1$predictions,b_test$status)\n",
    "\n",
    "#Benchmark - 2\n",
    "#Split the data set into positive and negative class and create a balanced data set and run the tests\n",
    "master_status_1 <- master[master$status == 1, ]\n",
    "master_status_0 <- master[master$status == 0, ]\n",
    "b_train_n <- master_status_0[sample(1:dim(master_status_0)[1],size = dim(master_status_1)[1]),]\n",
    "b_train_p <- master_status_1\n",
    "bal_master <- rbind(b_train_n,b_train_p)\n",
    "\n",
    "index <- sample(1:nrow(bal_master),round(0.8*nrow(bal_master)))\n",
    "b_train <- bal_master[index,]\n",
    "b_test <- bal_master[-index,]\n",
    "\n",
    "#remove unwanted variables\n",
    "rm(master_status_0,master_status_1,b_train_p,b_train_n,bal_master)\n",
    "\n",
    "#Run a sample random forest alogrithm on the balanced dataset. 50% positive & 50% negative\n",
    "rf.bench2 <- ranger(data = b_train[,3:24],dependent.variable.name = \"status\",num.trees = 300,write.forest = T)\n",
    "rf.pred2 <- predict(rf.bench2,data = b_test[,3:24])\n",
    "\n",
    "#performance check\n",
    "checkPerformance(rf.pred2$predictions,b_test$status)\n",
    "\n",
    "\n",
    "########### Outlier Treatment ############\n",
    "\n",
    "m <- master\n",
    "\n",
    "m[m$hits > 100,]$hits <- 100\n",
    "m[m$prevhits > 100,]$prevhits <- 100\n",
    "m[m$spamComplaints > 500,]$spamComplaints <- 500\n",
    "m[m$emailSent > 1000,]$emailSent <- 1000\n",
    "\n",
    "#Comment the line below to avoid outlier treatment\n",
    "master <- m\n",
    "\n",
    "############# Feature Creation #############\n",
    "\n",
    "#Create features based on the date\n",
    "d <- master$asOfDate\n",
    "d_day <- weekdays(d)\n",
    "d_is_weekend <- d_day %in% c(\"Saturday\",\"Sunday\")\n",
    "d_month <- months(d)\n",
    "d_weeknum <- week(d)\n",
    "\n",
    "#Did the rolling variables increase or decrease?Create a few features\n",
    "login_toggle <- master$currLogins > master$prevLogins\n",
    "ticket_toggle <- master$currTickets > master$prevTickets\n",
    "\n",
    "#Was the number of broadcasts & campaigns too low? Less than 20 contacts?\n",
    "broad_amount <- master$BroadCasts20_4weeks - master$broadcast4\n",
    "camp_amount <- master$Camp20_4weeks - master$Camp2_4weeks\n",
    "\n",
    "#Active contact ratio\n",
    "active_ratio <- master$ActiveDistinctContacts/(master$Contacts_4weeks+1)\n",
    "\n",
    "#Bind all these features to the dataframe\n",
    "\n",
    "master_feature <- cbind(master[,-2],d_day,d_is_weekend,d_month,d_weeknum,login_toggle,ticket_toggle,broad_amount,camp_amount,active_ratio)\n",
    "\n",
    "#Remove unwanted variables\n",
    "rm(d,d_weeknum,d_day,d_is_weekend,d_month,ticket_toggle,login_toggle,broad_amount,camp_amount,active_ratio)\n",
    "\n",
    "\n",
    "########## Initial Data Sampling  ##########\n",
    "\n",
    "\n",
    "index <- sample(1:nrow(master_feature),round(0.85*nrow(master_feature)))\n",
    "\n",
    "#Keep the 15% data as final test set\n",
    "final_test <- master_feature[-index,]\n",
    "\n",
    "#Rest 85% to be used for all training and validation\n",
    "train <- master_feature[index,]\n",
    "\n",
    "#Split the training data into positive and negative class\n",
    "train_p <- train[train$status == 1, ]\n",
    "train_n <- train[train$status == 0, ]\n",
    "\n",
    "#Sample as many negative class as the number of rows of the positive class\n",
    "train_nn <- train_n[sample(1:nrow(train_n),nrow(train_p)),]\n",
    "\n",
    "m_combined <- rbind(train_p,train_nn)\n",
    "\n",
    "#Create training and validation set from the combined set\n",
    "m_train_index <- sample(1:nrow(m_combined),0.8*nrow(m_combined))\n",
    "m_train <- m_combined[m_train_index,]\n",
    "m_val <- m_combined[-m_train_index,]\n",
    "\n",
    "\n",
    "############ Model Building ########\n",
    "\n",
    "#Model 1: random forest on the balanced dataset. 50% positive & 50% negative\n",
    "\n",
    "model_1_rf <- function(train, val, n_trees = 300) {\n",
    "rf.trial1 <- ranger(data = train[,-1],dependent.variable.name = \"status\",num.trees = n_trees,importance = \"impurity\",write.forest = T)\n",
    "rf.trail1.val <- predict(rf.trial1,data = val[,-1])\n",
    "\n",
    "#Returning the model and the validation characteristics\n",
    "return(c(checkPerformance(rf.trail1.val$predictions,val$status),rf.trial1))\n",
    "\n",
    "}\n",
    "\n",
    "rf_model <- model_1_rf(m_train,m_val,n_trees = 1000)\n",
    "\n",
    "\n",
    "#Model 2: xgboost on the balanced dataset. Converting everything to numerics before trial\n",
    "\n",
    "model_2_xgb <- function(train,val,depth = 50,eta = 0.1,nrounds = 300) {\n",
    "      \n",
    "  #Create numerical dataframe excluding the unique ID and the status variable\n",
    "  num_m_train <- sapply(m_train[,-which(colnames(m_train) %in% c(\"status\",\"V1\"))], function(x) as.numeric(x))\n",
    "  num_m_val <- sapply(m_val[,-which(colnames(m_train)  %in% c(\"status\",\"V1\"))], function(x) as.numeric(x))\n",
    "  \n",
    "  #Make xgb.DMatrix\n",
    "  dtrain <- xgb.DMatrix(data = num_m_train,label = as.numeric(as.character(m_train$status)))\n",
    "  dtest <-  xgb.DMatrix(data = num_m_val, label = as.numeric(as.character(m_val$status)))\n",
    "  \n",
    "  #Parameters for Classification using xgBoost\n",
    "  param_cls <- list(  objective           = \"binary:logistic\", \n",
    "                      # booster = \"gblinear\",\n",
    "                      eta                 = eta,\n",
    "                      max_depth           = depth,  # changed from default of 6\n",
    "                      subsample           = 1,\n",
    "                      colsample_bytree    = 1,\n",
    "                      eval_metric         = \"error\"\n",
    "                      # alpha = 0.0001, \n",
    "                      # lambda = 1\n",
    "  )\n",
    "  \n",
    "  #Create the models\n",
    "  xgb_model_cls <- xgb.train(   params              = param_cls, \n",
    "                                data                = dtrain,\n",
    "                                nrounds             = nrounds, # changed from 300\n",
    "                                verbose = 2,\n",
    "                                nthreads            = 8,\n",
    "                                watchlist = list(validation = dtest)\n",
    "                                \n",
    "  )\n",
    "  \n",
    "  xgb_pred <- xgboost::predict(xgb_model_cls, dtest)\n",
    "  xgb_pred <- sapply(xgb_pred , function(x) ifelse(x < 0.5, x <- 0, x <- 1))\n",
    "  \n",
    "\n",
    "  #Uncomment the line below to calculate feature importance\n",
    "  #xgb_imp <- xgb.importance(model = xgb_model_cls,feature_names = attributes(num_m_train)$dimnames[[2]])\n",
    "  \n",
    "  #Return the model and performance measures\n",
    "  return(c(checkPerformance(as.factor(xgb_pred),as.factor(m_val$status)),xgb_model_cls))\n",
    "}\n",
    "  \n",
    "#Call the xgb model on the initial sample created\n",
    "xgb <- model_2_xgb(m_train,m_val,depth = 170, nrounds = 100, eta = 0.02)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######## Customised Undersampling ########\n",
    "#Earlier sampling was a selection of 50% random samples form the positive and negative class.\n",
    "#Lot of information from the negative class lost in this case.\n",
    "#So, create several datasets with balanced distribution, apply the models on them,ensemble the models\n",
    "\n",
    "\n",
    "#Keep 15% of number of rows of positive class for validation set \n",
    "index_n <- sample(1:nrow(train_n),size = 0.15*nrow(train_p))\n",
    "index_p <- sample(1:nrow(train_p),size = 0.15*nrow(train_p))\n",
    "\n",
    "test_n <- train_n[index_n,]\n",
    "test_p <- train_p[index_p,]\n",
    "\n",
    "#Remaining negative class would be split up\n",
    "train_n <- train_n[-index_n,]\n",
    "train_p <- train_p[-index_p,]\n",
    "\n",
    "test_np <- rbind(test_n,test_p)\n",
    "\n",
    "nrow_p <- nrow(train_p)\n",
    "nrow_n <- nrow(train_n)\n",
    "\n",
    "\n",
    "#nrow_p number of samples have to selected multiple times without repetation\n",
    "n_sets <- ceiling(nrow_n/nrow_p)\n",
    "\n",
    "#number of rows in each subset\n",
    "samplesize <- ceiling(nrow_n/n_sets)\n",
    "nrow_subset_1 <- 1\n",
    "\n",
    "#initialising a list of vectors\n",
    "list_of_trainsets <- vector(\"list\",n_sets )\n",
    "for(i in 1:n_sets){\n",
    "  if(i*samplesize > nrow_n) {\n",
    "    nrow_subset_2 <- nrow_n\n",
    "  }\n",
    "  else {\n",
    "    nrow_subset_2 <- i*samplesize\n",
    "  }\n",
    "  \n",
    "  #range of rows is betweeb nrow_subset_1 and nrow_subset_2\n",
    "  list_of_trainsets[[i]] <- rbind(train_n[nrow_subset_1:nrow_subset_2,],train_p)\n",
    "  nrow_subset_1 <- nrow_subset_2+1\n",
    "}\n",
    "\n",
    "##Model execution on subsets using both RF and xgb\n",
    "\n",
    "list_of_models_rf <- vector(\"list\",n_sets)\n",
    "list_of_models_xgb <- vector(\"list\",n_sets)\n",
    "\n",
    "for(i in 1:(n_sets)){\n",
    "  \n",
    "  list_of_models_rf[[i]] <- model_1_rf(list_of_trainsets[[i]],test_np,n_trees = 1000)\n",
    "  list_of_models_xgb[[i]] <- model_2_xgb(list_of_trainsets[[i]],nrounds = 500,eta = 0.02)\n",
    "}\n",
    "\n",
    "\n",
    "######## Ensembling models ####### #TBD#\n",
    "\n",
    "pred_list_rf <- vector(\"list\",n_sets)\n",
    "\n",
    "for (i in 1:n_sets){\n",
    "  pred_list_rf[[i]] <- predict(list_of_models_rf[[i]]$forest,data = final_test)$predictions\n",
    "}\n",
    "\n",
    "pred_list_xgb <- vector(\"list\",n_sets)\n",
    "\n",
    "for (i in 1:n_sets){\n",
    "  num_final_test <- sapply(final_test[,-which(colnames(final_test) == \"status\")], function(x) as.numeric(x))\n",
    "  fdtest <- xgb.DMatrix(data = num_final_test, label = as.numeric(as.character(final_test$status)))\n",
    "  pred_list_xgb[[i]] <- predict(list_of_models_xgb[[i]]$handle,fdtest)\n",
    "  pred_list_xgb[[i]]<- sapply(pred_list_xgb[[i]] , function(x) ifelse(x < 0.5, x <- 0, x <- 1))\n",
    "}\n",
    "\n",
    "\n",
    "n <- length(pred_list[[1]])\n",
    "DF1 <- structure(pred_list_rf, row.names = c(NA, -n), class = \"data.frame\")\n",
    "colnames(DF1) <- c('1','2','3','4','5','6','7','8','9','10','11','12','13','14')\n",
    "\n",
    "DF2 <- structure(pred_xgb, row.names = c(NA, -n), class = \"data.frame\")\n",
    "colnames(DF2) <- c('1','2','3','4','5','6','7','8','9','10','11','12','13','14')\n",
    "\n",
    "DFNew1 <- sapply(DF1,function(x) as.numeric(levels(x)[x]))\n",
    "DFNew2 <- sapply(DF2,function(x) as.numeric(levels(x)[x]))\n",
    "\n",
    "DF <- cbind(DFNew1,DFNew2)\n",
    "val <- rowSums(DF)\n",
    "#If majority of predictors suggests postive class, then final = TRUE\n",
    "final <- as.factor(val > n_sets)\n",
    "\n",
    "checkPerformance(final,as.factor(final_test$status == 1))\n",
    "\n",
    "\n",
    "######## Final Test ###########\n",
    "num_final_test <- sapply(final_test[,-which(colnames(final_test) == \"status\")], function(x) as.numeric(x))\n",
    "fdtest <- xgb.DMatrix(data = num_final_test, label = as.numeric(as.character(final_test$status)))\n",
    "xgb_pred <- xgboost::predict(xgb$handle, fdtest )\n",
    "xgb_pred <- sapply(xgb_pred , function(x) ifelse(x < 0.5, x <- 0, x <- 1))\n",
    "\n",
    "checkPerformance(as.factor(xgb_pred),as.factor(final_test$status))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
